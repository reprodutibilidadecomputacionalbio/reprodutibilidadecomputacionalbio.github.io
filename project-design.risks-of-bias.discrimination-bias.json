{"version":1,"kind":"Article","sha256":"bedadd8a068235bd2d9740e331c17bc8b36b4a1aaa26a6bddb6fe375ba2915c0","slug":"project-design.risks-of-bias.discrimination-bias","location":"/project-design/risks-of-bias/discrimination-bias.md","dependencies":[],"frontmatter":{"title":"Discrimination and Bias","content_includes_title":false,"authors":[{"nameParsed":{"literal":"The Turing Way Community","given":"The Turing Way","family":"Community"},"name":"The Turing Way Community","id":"contributors-myst-generated-uid-0"}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/the-turing-way/the-turing-way","copyright":"2019-2025","numbering":{"title":{"offset":2}},"edit_url":"https://github.com/the-turing-way/the-turing-way/blob/main/book/website/project-design/risks-of-bias/discrimination-bias.md","exports":[{"format":"md","filename":"discrimination-bias.md","url":"/build/discrimination-bias-3d2f4d3e1bf317bae08eb9b460933ed2.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The word ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ieECzWU69F"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"bias","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"szaw4Qa5Ey"}],"key":"C6WVxGcpKr"},{"type":"text","value":" is often used to refer to prejudice against a group of people.\nWhile this differs from the statistical bias which has been the focus of this chapter, there is significant overlap between these types of bias.\nA model may act in a biased way against a group of people, which can lead to discriminatory outcomes.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"x2RfM9YpXn"}],"key":"TlY2HIS2vT"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Data science and AI will always be at risk of bias - biased data leads to biased models, which may lead to biased decision-making.\nDatasets contain biases which reflect the personal and societal biases which were present during data collection.\nThey also become biased due to difficulties in ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"feFZhMO7zS"},{"type":"crossReference","kind":"heading","identifier":"pd-risks-of-bias-identifying-risks-of-bias-sampling","label":"pd-risks-of-bias-identifying-risks-of-bias-sampling","children":[{"type":"text","value":"sampling","key":"ofEAfenTGo"}],"template":"{name}","resolved":true,"html_id":"pd-risks-of-bias-identifying-risks-of-bias-sampling","remote":true,"url":"/project-design/risks-of-bias/identifying-risks-of-bias","dataUrl":"/project-design.risks-of-bias.identifying-risks-of-bias.json","key":"P7D3MedlOx"},{"type":"text","value":".","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"qilW6n1tfd"}],"key":"uhSF2FQ8NV"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Examples of discrimination in AI include ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"x61tkmOVTn"},{"type":"link","url":"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"racism in predictive policing","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Hvz420ZaQg"}],"urlSource":"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/","key":"cdP23UfDCP"},{"type":"text","value":", ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"AfsI2tiVAa"},{"type":"link","url":"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"sexism in recruiting","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"HEnrj2vwAM"}],"urlSource":"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/","key":"aDyxHrlpyM"},{"type":"text","value":", and multiple forms of ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"A5kvDt6pd3"},{"type":"link","url":"https://en.wikipedia.org/wiki/Tay_(chatbot)","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"discrimination in generative AI","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"EdOlgCAs6y"}],"urlSource":"https://en.wikipedia.org/wiki/Tay_(chatbot)","data":{"page":"Tay_(chatbot)","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"SZW1BrUKZ4"},{"type":"text","value":".\nThese algorithms form negative feedback loops, with historic discrimination contributing to discrimination in future decision-making.\nThere is ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"sulEkPhMJA"},{"type":"cite","url":"https://dl.acm.org/doi/10.1145/3616865","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"no one-size fits all solution","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"sOq0gURh4u"}],"kind":"narrative","label":"Caton_2024","identifier":"https://dl.acm.org/doi/10.1145/3616865","enumerator":"1","key":"Bu9ROeTEt7"},{"type":"text","value":" to ensure fairness in AI, though guidance is available through the ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"gAkKIdsd4R"},{"type":"link","url":"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Information Commissioner’s Office (ICO)","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"yY39xZbfPv"}],"urlSource":"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/","key":"wzQgAehXoQ"},{"type":"text","value":" and the ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"e9yyubZzF1"},{"type":"link","url":"https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-fairness-practice","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Alan Turing Institute","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Jwsnh0OSSp"}],"urlSource":"https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-fairness-practice","key":"vrSAj0YZGz"},{"type":"text","value":".","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"e6L1jidHlS"}],"key":"VmPNj2sKxF"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Further information can also be found through the ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"HX7hyqlhMC"},{"type":"link","url":"https://www.youtube.com/watch?v=Tuz7IGqDAIs&list=PLDbZND-EA4eHYGwDyOEiUumyCNPAx2gyF","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Turing-Roche Knowledge Share Event: Fairness in AI for Health","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"TUQamGfKJI"}],"urlSource":"https://www.youtube.com/watch?v=Tuz7IGqDAIs&list=PLDbZND-EA4eHYGwDyOEiUumyCNPAx2gyF","key":"xZWTfKYzXU"},{"type":"text","value":".","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"e8EM8jBUT4"}],"key":"CR0OKS41PC"}],"key":"h7a4vdXuHl"}],"key":"ngou14V9ah"},"references":{"cite":{"order":["Caton_2024"],"data":{"Caton_2024":{"label":"Caton_2024","enumerator":"1","doi":"10.1145/3616865","html":"Caton, S., & Haas, C. (2024). Fairness in Machine Learning: A Survey. <i>ACM Computing Surveys</i>, <i>56</i>(7), 1–38. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1145/3616865\">10.1145/3616865</a>","url":"https://doi.org/10.1145/3616865"}}}},"footer":{"navigation":{"prev":{"title":"Validation and Generalisability","url":"/project-design/risks-of-bias/validation-generalisability","group":"The Turing Way"},"next":{"title":"Guide for Communication","url":"/communication/communication","group":"The Turing Way"}}},"domain":"http://localhost:3001"}