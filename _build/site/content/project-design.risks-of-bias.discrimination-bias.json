{"version":1,"kind":"Article","sha256":"bedadd8a068235bd2d9740e331c17bc8b36b4a1aaa26a6bddb6fe375ba2915c0","slug":"project-design.risks-of-bias.discrimination-bias","location":"/project-design/risks-of-bias/discrimination-bias.md","dependencies":[],"frontmatter":{"title":"Discrimination and Bias","content_includes_title":false,"authors":[{"nameParsed":{"literal":"The Turing Way Community","given":"The Turing Way","family":"Community"},"name":"The Turing Way Community","id":"contributors-myst-generated-uid-0"}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/the-turing-way/the-turing-way","copyright":"2019-2025","numbering":{"title":{"offset":2}},"edit_url":"https://github.com/the-turing-way/the-turing-way/blob/main/book/website/project-design/risks-of-bias/discrimination-bias.md","exports":[{"format":"md","filename":"discrimination-bias.md","url":"/discrimination-bias-3d2f4d3e1bf317bae08eb9b460933ed2.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"The word ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"M3LPM5Fjru"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"bias","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"rKk2JFyfki"}],"key":"gmFOF0Uw3p"},{"type":"text","value":" is often used to refer to prejudice against a group of people.\nWhile this differs from the statistical bias which has been the focus of this chapter, there is significant overlap between these types of bias.\nA model may act in a biased way against a group of people, which can lead to discriminatory outcomes.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"g8ZV0FawZy"}],"key":"oun8RW5bKi"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Data science and AI will always be at risk of bias - biased data leads to biased models, which may lead to biased decision-making.\nDatasets contain biases which reflect the personal and societal biases which were present during data collection.\nThey also become biased due to difficulties in ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"UYL5vrkcp9"},{"type":"crossReference","kind":"heading","identifier":"pd-risks-of-bias-identifying-risks-of-bias-sampling","label":"pd-risks-of-bias-identifying-risks-of-bias-sampling","children":[{"type":"text","value":"sampling","key":"DdNs8lo2yq"}],"template":"{name}","resolved":true,"html_id":"pd-risks-of-bias-identifying-risks-of-bias-sampling","remote":true,"url":"/project-design/risks-of-bias/identifying-risks-of-bias","dataUrl":"/project-design.risks-of-bias.identifying-risks-of-bias.json","key":"Egu8LW2Nrx"},{"type":"text","value":".","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"c0wgnaSBuF"}],"key":"E6H97FSq5p"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Examples of discrimination in AI include ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"B647upbAQ2"},{"type":"link","url":"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"racism in predictive policing","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Js4fKdwyDK"}],"urlSource":"https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/","key":"B2Nxk8asp0"},{"type":"text","value":", ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"dzha9d8Gfo"},{"type":"link","url":"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"sexism in recruiting","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"gXvvGEdT1X"}],"urlSource":"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/","key":"aHclW4zuEC"},{"type":"text","value":", and multiple forms of ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"chRVLplnkP"},{"type":"link","url":"https://en.wikipedia.org/wiki/Tay_(chatbot)","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"discrimination in generative AI","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"LX2cbziMuC"}],"urlSource":"https://en.wikipedia.org/wiki/Tay_(chatbot)","data":{"page":"Tay_(chatbot)","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"VtdQRPRhiJ"},{"type":"text","value":".\nThese algorithms form negative feedback loops, with historic discrimination contributing to discrimination in future decision-making.\nThere is ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hKHxlMp7C3"},{"type":"cite","url":"https://dl.acm.org/doi/10.1145/3616865","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"no one-size fits all solution","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"QtrqvWLETv"}],"kind":"narrative","label":"Caton_2024","identifier":"https://dl.acm.org/doi/10.1145/3616865","enumerator":"1","key":"ncUkUGfNW3"},{"type":"text","value":" to ensure fairness in AI, though guidance is available through the ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"wC9q4ys4i1"},{"type":"link","url":"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Information Commissioner’s Office (ICO)","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"IlWFGBik5j"}],"urlSource":"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/","key":"ElX9yCTE1x"},{"type":"text","value":" and the ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"k8hlPJpL3G"},{"type":"link","url":"https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-fairness-practice","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Alan Turing Institute","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"WbKYKQImYW"}],"urlSource":"https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-fairness-practice","key":"kEaZwH4eoy"},{"type":"text","value":".","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"F4Aensmn2M"}],"key":"C9awGDMqej"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Further information can also be found through the ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"ayJ1OIrTi3"},{"type":"link","url":"https://www.youtube.com/watch?v=Tuz7IGqDAIs&list=PLDbZND-EA4eHYGwDyOEiUumyCNPAx2gyF","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Turing-Roche Knowledge Share Event: Fairness in AI for Health","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"sXJm2iLZ6y"}],"urlSource":"https://www.youtube.com/watch?v=Tuz7IGqDAIs&list=PLDbZND-EA4eHYGwDyOEiUumyCNPAx2gyF","key":"liBwJThDpS"},{"type":"text","value":".","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"E1j891fDrY"}],"key":"ToVeoTwg7E"}],"key":"l45VSPPYqG"}],"key":"aYPhJBATrP"},"references":{"cite":{"order":["Caton_2024"],"data":{"Caton_2024":{"label":"Caton_2024","enumerator":"1","doi":"10.1145/3616865","html":"Caton, S., & Haas, C. (2024). Fairness in Machine Learning: A Survey. <i>ACM Computing Surveys</i>, <i>56</i>(7), 1–38. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1145/3616865\">10.1145/3616865</a>","url":"https://doi.org/10.1145/3616865"}}}}}